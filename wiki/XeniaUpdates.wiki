#summary project update notes

= SQLiteGeo (April 8, 2008) =

There is a geospatially enabled version of Sqilte labeled SQLiteGeo available at http://www.gaia-gis.it/spatialite Would recommend using this version of sqlite for its geospatial and shapefile support.  OGR also supports sqlite access, see http://www.jasonbirch.com/nodes/2008/05/06/184/sqlite-for-fdo-with-sugar-free-ogr/

= monthly/latest file db replication script (April 10, 2008) =

This [http://code.google.com/p/xenia/source/browse/trunk/sqlite/monthly_latest_copy source code] shows how I'm using my [http://www.carocoops.org/obskml/feeds/xenia/archive/latest.sql latest.sql] hourly feed to create/populate [http://carocoops.org/seacoos_data/sqlite monthly archive file db's and a latest db].  The amount total amount of observation data currently collected is around 20 MB per day, so a month file should be around 20 MB*30 days = 600 MB

= sqlite version 3 (May 31, 2008) =

Simplified the sqlite schema [http://www.carocoops.org/obskml/microwfs/db_xenia_v3_sqlite.sql main schema] by removing tables which have been unutilized thus far (quality_control, collection) and adding a 'metadata' table which provides a simple xml file lookup mechanism associated with various table row_id's.  On the pro-side this means that platform, sensor, qc, etc configs can change independently of the basic RDB schema, on the con side I'll be depending on a limited set of xml schema conventions and hardcoding to these various schemas(as opposed to hardtabling I guess).

The metadata table is just a simple file pointer to supporting descriptive xml files of certain useful xml schema conventions.  The same metadata_id could appear on several rows as a way of providing an archival trail of changes to the same type file using the fields 'begin_date' and 'end_date' to determine the effective date range and 'active' to determine the latest (and only) active metadata record.  By this way, the database reference integrity is preserved while allowing simpler files and their attributes to change over time.

My current thinking regarding the metadata xml files is to combine metadata for platforms,sensors, qc to one file although this is somewhat inefficient as one change would effect the entire file, one file per platform seems to be the usual convention thus far.  Also thinking to use a filename convention of filename_begindate_enddate.xml 

The earlier 'collection' table could be referenced as a sub element schema structure within each of these other parent schemas.

Also removed 'm_desc' from the multi_obs table.

= code revision 238 (August 26, 2008) =

Adding/changing several things with this revision detailed below - all of these changes are in relation to the sqlite database version of the xenia code.

== sqlite_query.pl ==

http://code.google.com/p/xenia/source/browse/trunk/sqlite/misc/sqlite_query.pl

This is a general use query script for performing many queries in a batch manner using q [http://code.google.com/p/xenia/source/browse/trunk/sqlite/misc/query.txt query.txt] file which specifies the 

== batch_insert.pl ==

http://code.google.com/p/xenia/source/browse/trunk/sqlite/misc/batch_insert.pl

After noticing gaps in the data and investigating the insert process for the sqlite xenia database, it was seen that the sqlite database would become 'locked' and inserts to the database during this locked period were missed/unperformed resulting in gaps in the database data.

While sqlite allows multiple concurrent readers to the same file, only one process is allowed to write to the database at a time.  A writer process can be made to 'wait' for a specified timeout period via either the meta commands (.timeout 30000 would be a 30 second timeout) or via a similar type request in the perl dbh (database handler) setup commands.  

A bug with running sqlite in a batch mode is that the meta command (.timeout 30000) seemed to always be ignored whereas the dbh setup command was respected.  This resulted in changing the existing database write commands from something like

$path_sqlite $dbname < $sql_file

to

perl batch_insert.pl $dbname $sql_file

The batch_insert.pl script runs each sql file INSERT command one at a time in a non-transaction mode against the database with a wait/timeout condition(60 seconds) and will continue to retry the INSERT command until the the database is no longer returning a 'locked' status in the error code.

By passing the earlier sql batch files through the batch_insert.pl script, this is having the desired effect of insuring that all data INSERT requests are handled properly.  I would like to also nest these sql batches as a sql transaction (begin transaction; commit; ) but the gain in speed doesn't offset the advantage of having a few individual INSERT commands fail where there might be bad data versus the whole transaction being rejected.
 
=  =