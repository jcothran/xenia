#summary project update notes

= SQLiteGeo (April 8, 2008) =

There is a geospatially enabled version of Sqilte labeled SQLiteGeo available at http://www.gaia-gis.it/spatialite Would recommend using this version of sqlite for its geospatial and shapefile support.  OGR also supports sqlite access, see http://www.jasonbirch.com/nodes/2008/05/06/184/sqlite-for-fdo-with-sugar-free-ogr/

= monthly/latest file db replication script (April 10, 2008) =

This [http://code.google.com/p/xenia/source/browse/trunk/sqlite/monthly_latest_copy source code] shows how I'm using my [http://www.carocoops.org/obskml/feeds/xenia/archive/latest.sql latest.sql] hourly feed to create/populate [http://carocoops.org/seacoos_data/sqlite monthly archive file db's and a latest db].  The amount total amount of observation data currently collected is around 20 MB per day, so a month file should be around 20 MB*30 days = 600 MB

= sqlite version 3 (May 31, 2008) =

Simplified the sqlite schema [http://www.carocoops.org/obskml/microwfs/db_xenia_v3_sqlite.sql main schema] by removing tables which have been unutilized thus far (quality_control, collection) and adding a 'metadata' table which provides a simple xml file lookup mechanism associated with various table row_id's.  On the pro-side this means that platform, sensor, qc, etc configs can change independently of the basic RDB schema, on the con side I'll be depending on a limited set of xml schema conventions and hardcoding to these various schemas(as opposed to hardtabling I guess).

The metadata table is just a simple file pointer to supporting descriptive xml files of certain useful xml schema conventions.  The same metadata_id could appear on several rows as a way of providing an archival trail of changes to the same type file using the fields 'begin_date' and 'end_date' to determine the effective date range and 'active' to determine the latest (and only) active metadata record.  By this way, the database reference integrity is preserved while allowing simpler files and their attributes to change over time.

My current thinking regarding the metadata xml files is to combine metadata for platforms,sensors, qc to one file although this is somewhat inefficient as one change would effect the entire file, one file per platform seems to be the usual convention thus far.  Also thinking to use a filename convention of filename_begindate_enddate.xml 

The earlier 'collection' table could be referenced as a sub element schema structure within each of these other parent schemas.

Also removed 'm_desc' from the multi_obs table.

= code revision 238 (August 26, 2008) =

Adding/changing several things with this revision detailed below - all of these changes are in relation to the sqlite database version of the xenia code.

== sqlite_query.pl ==

http://code.google.com/p/xenia/source/browse/trunk/sqlite/misc/sqlite_query.pl

This is a general use query script for performing many queries in a batch manner using a [http://code.google.com/p/xenia/source/browse/trunk/sqlite/misc/query.txt query.txt] file which specifies in tab separated fields:filename_out dbname sql_query

== batch_insert.pl ==

http://code.google.com/p/xenia/source/browse/trunk/sqlite/misc/batch_insert.pl

After noticing gaps in the data and investigating the insert process for the sqlite xenia database, it was seen that the sqlite database would become 'locked' and inserts to the database during this locked period were missed/unperformed resulting in gaps in the database data.

While sqlite allows multiple concurrent readers to the same file, only one process is allowed to write to the database at a time.  A writer process can be made to 'wait' for a specified timeout period via either the meta commands (.timeout 30000 would be a 30 second timeout) or via a similar type request in the perl dbh (database handler) setup commands.  

A bug with running sqlite in a batch mode is that the meta command (.timeout 30000) seemed to always be ignored whereas the dbh setup command was respected.  This resulted in changing the existing database write commands in several of the perl scripts from something like

{{{$path_sqlite $dbname < $sql_file}}}

to

{{{perl batch_insert.pl $dbname $sql_file}}}

The batch_insert.pl script runs each sql file INSERT command one at a time in a non-transaction mode against the database with a wait/timeout condition(60 seconds) and will continue to retry the INSERT command until the the database is no longer returning a 'locked' status in the error code.

By passing the earlier sql batch files through the batch_insert.pl script, this is having the desired effect of insuring that all data INSERT requests are handled properly.  I would like to also nest these sql batches as a sql transaction (begin transaction; commit; ) but the gain in speed doesn't offset the advantage of having a few individual INSERT commands fail where there might be bad data versus the whole transaction being rejected.

== microwfsFlow.sh ==

http://code.google.com/p/xenia/source/browse/trunk/sqlite/crontab/microwfsFlow.sh

This shell script is called by the crontab at 50 minutes past each hour and details the various subscripts called and products generated.

The name 'microwfs' is somewhat of a misnomer - the sqlite version of things started in the context of the MicroWFS experiment and the naming convention is sticking with it for now.

== top of the hour trigger ==

http://code.google.com/p/xenia/source/browse/trunk/sqlite/sql/top_report_hour.sql

Spent a while trying to develop a sqlite version of the [http://nautilus.baruch.sc.edu/twiki_dmcc/bin/view/Main/XeniaPackageV2#d_top_of_hour_integer_d_report_h  postgres version] of this trigger, but was very difficult to do as sqlite does not currently support variables or if/then conditions in triggers similar to postgres triggers.  Tried breaking everything into conditional update statements and while I got a working version, it became very verbose and *SLOW* so not implemented.  If I take another try at this will either implement as a C compiled function called from the trigger or push data to a separate 'top of hour only' database.

== flow monitor ==

http://code.google.com/p/xenia/source/browse/trunk/sqlite/flow_monitor

The shell script [http://code.google.com/p/xenia/source/browse/trunk/sqlite/flow_monitor/checkStatus.sh checkStatus.sh] is called every 10 minutes past the hour and the script [http://code.google.com/p/xenia/source/browse/trunk/sqlite/flow_monitor/check_status.pl check_status.pl] uses the command line parameters to perform a database query on the latest sensor counts per organization.  The query results are sent to a column-oriented text file which is used to produce an hourly updated graph of the results using gnuplot ( [http://code.google.com/p/xenia/source/browse/trunk/sqlite/flow_monitor/status.lib status.lib] [http://code.google.com/p/xenia/source/browse/trunk/sqlite/flow_monitor/graphCommon.lib graphCommon.lib]) and an email notification is sent to staff if the sensor counts are critically low(zero for any organization with exceptions for those which are chronically low).

The three status graphs are currently:

Latest aggregated observations

http://carocoops.org/obskml/scripts/flow_microwfs.png

Latest aggregated observation for counts 0-100 (a magnification of the above graph at the bottom where organizations may only be providing a handful of observations each)

http://carocoops.org/obskml/scripts/flow_low.png

A 10 hour delayed offset(graph should be the same as the flow_microwfs.png graph except 10 hours lagging) of the latest observations aggregated to the julian weekly and monthly databases at http://www.carocoops.org/seacoos_data/sqlite/

http://carocoops.org/obskml/scripts/flow_archive.png

== youtube demo video ==

Put together an initial [http://www.youtube.com/watch?v=_CO9o2DyVV0 YouTube tech demo video] - captured the screen/audio using freeware 'CamStudio' but in trying to convert this to 640x480 screen size for youtube the detailed readouts are blurry/unreadable and the audio kept lagging behind the video becoming further out of sync as the video progresses.  Will probably do a retake of this with my initial screen resolution at 800x600 (instead of 1280x1024) to get more legible results.