#summary Scripts that create and import the data into the Xenia database.

<wiki:toc max_depth="4" />

= Introduction =

These are the scripts currently running that retrieve the data from various sources and import that data into the Xenia database.<br/>


==Machine== 
neptune.baruch.sc.edu <br/>


== Scripts ==
----
=== NetCDF Scout ===
*Script*: GetLatestData.pl <br/>
*Command line parameters*: <br/>
{{{
--URLControlFile is the XML file with the URL list the script will download the files from.
--FileFilter is the filter to apply to the file listings to allow the script to download the files we really want. Optional.
--DirForObsKml provides the path to the store the ObsKML files created. This is an optional argument, if not provided no ObsKML files are written.             
--Delete specifies we delete the netcdf files after we write the ObsKML files. This is an optional argument.
--NetCDFDir is the directory to store the downloaded files. Optional, default is ./latest_netcdf_files.
--FetchLogDir is the directory were the file time stamps are stored. The script uses these files to determine which files are really the latest. Optional, default is ./fetch_logs.             
 --UseLastNTimeStamps Integer representing the last N time entries to use when converting the data to obsKML. Optional.
--UseLastNTimeStampsForOrg Optional. List specifing orginization and the last N time entries to use for them. Whereas the UseLastNTimeStamps is applied to every organization, this can tailor to the specific. If not provided and UseLastNTimeStamps is provided, it is used. 
}}}
<br/>
*Directory*: /home/xeniaprod/scripts/scout/trunk<br/>
*Description*: Process netCDF files converting them into obsKML which are then processed by obskml_to_xenia_postgresql.pl to store the data into the database. This is scheduled to run in the crontab. NetCDF files for Carolinas RCOOS, NDBC, USF, NWS, NOS, and NCCOOS are processed. The obsKML files are then pulled into the database using the following scripts:<br/>
*Schedule*: Runs twice an hour.
----

=== Carocoops NetCDF Scout ===
*Script*: populate_xenia_carocoops.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the Carocoops netcdf files. This script incoporates the NetCDF scout above, but only handles the Carocoops 
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
  
  
=== CORMP NetCDF Scout ===
*Script*: populate_xenia_cormp.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the CORMP netcdf files. This script incoporates the NetCDF scout above, but only handles the CORMP 
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
 

=== USF NetCDF Scout ===
*Script*: populate_xenia_usf.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the USF and NDBC netcdf files. This script incoporates the NetCDF scout above, but only handles the USF and NDBC
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
         

=== NWS Processing ===
*Script*: populate_xenia_nws <br/>
*Directory*: /home/xeniaprod/scripts/postgresql/feeds/federal/nws <br/>
*Description*: Pulls in NWS data directly from the NWS site. This is used in tandem with the NetCDF scout. Currently this is scheduled to run in crontab. <br/>
*Shell Script Makeup* <br/>
	_Script_: mk_sql_for_xenia.pl <br/>
	_Command line parameters_: <br/>
		Argument 1: Provider name – nos, nws, etc <br/>
		Argument 2: Not used <br/>
		Argument 3: Fully qualified path to SQL output file <br/>
	_Description_: Connects to the NWS webservice, pulls the data down into a SQL file for importation into the database. Next psql is executed to import the SQL 	file.<br/>
*Schedule*: Runs every 10 minutes. NWS stations have a fairly rapid update rate.
----


=== NOS Processing ===
*Script*: populate_xenia_nos <br/>
*Directory*: /home/xeniaprod/scripts/postgresql/feeds/federal/nos <br/>
*Description*: Pulls in NOS data directly from the NOS site. This is used in tandem with the NetCDF scout. Currently this is scheduled to run in crontab. <br/>
*Shell Script Makeup* <br/>
	_Script_: mk_sql_for_xenia.pl <br/>
	_Command line parameters_: <br/>
		Argument 1: Provider name – nos, nws, etc <br/>
		Argument 2: Not used <br/>
		Argument 3: Fully qualified path to SQL output file	<br/>
	_Description_: Connects to the NOS webservice, pulls the data down into a SQL 	file for importation into the database. Next psql is executed to import the SQL 	file. <br/>
*Schedule*: Runs twice an hour.
----

=== USGS Processing ===
*Script*: populate_xenia_usgs <br/>
*Directory*: /home/xeniaprod/scripts/postgresql/feeds/federal/usgs<br/>
*Description*: Pulls in USGS data directly from the USGS site.  Currently this is scheduled to run in crontab. Cleans up previous KML, KMZ, and SQL files.<br/>
*Shell Script Makeup* <br/>
	_Script_: gen_wq_obskml.pl <br/>
	_Command line parameters_: <br/>
		Argument 1: Provider name – usgs <br/>
		Argument 2: Base output directory. Argument one is appended to complete the directory. <br/>
		Argument 3: XML platform list that details the platforms to query.<br/>
	_Description_: Connects to the USGS webservice, pulls the data down into a KML 	file for importation into the database.<br/>
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>

*Schedule*: 45 minutes after the hour.
----

=== FIT Processing ===
*Script*: feed_fit.sh <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Shell script that pulls in data from the Sebastion Inlet station in Florida. <br/>
*Shell Script Makeup* <br/>
	_Script_: fit_to_obskml.pl <br/>
	_Directory_: /home/xeniaprod/scripts/postgresql/feeds/fit <br/>
	_Description_: Process the Fit data. <br/>
	_Script_: obskml_to_xenia_postgresql.pl <br/>
	_Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
	_Description_: Takes the obsKML and writes it into the database. <br/>
*Schedule*: Runs every quarter hour.
----


=== YSI Apache Pier Processing ===
*Script*: getYSIData.py <br/>
*Command line parameters*: <br/>
	Argument 1: /home/xeniaprod/config/ysiParseConfig.xml <br/>
*Directory*: /home/xeniaprod/ /scripts/postgresql/feeds/ysi <br/>
*Description*: Pulls in the data for the YSI site which is described in the configuration file 
provided  on the command line. This script pulls in the data for the Apache Pier. <br/>
*Schedule*: Runs twice an hour.
----

=== YSI Nerrs Processing ===
*Script*: getYSIData.py <br/>
*Command line parameters*: <br/>
	Argument 1: /home/xeniaprod/config/nerrsYSIConfigLinux.xml <br/>
*Directory*: /home/xeniaprod/ /scripts/postgresql/feeds/ysi <br/>
*Description*: Pulls in the data for the YSI site which is described in the configuration file 
provided  on the command line. This script pulls in the data for the some NERRS water quality sites. <br/>
*Schedule*: Runs twice an hour.
----

== Machine ==
nautilus.baruch.sc.edu

== Scripts ==

----
*Process*: <br/>
*Script*: <br/>
*Command line parameters*: <br/>
*Directory*:  <br/>
*Description*:  <br/>
*Schedule*: 