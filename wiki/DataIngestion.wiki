#summary Scripts that create and import the data into the Xenia database.

<wiki:toc max_depth="4" />

= Introduction =

These are the scripts currently running that retrieve the data from various sources and import that data into the Xenia database.<br/>


= Machine: Neptune = 
neptune.baruch.sc.edu <br/>


== Scripts Neptune ==
----
=== NetCDF Scout ===
*Script*: populate_xenia_netcdfscout.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing netcdf files that aren't handled through the other RA specific processing scripts below. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Directory_: /home/xeniaprod/scripts/scout/trunk <br/>
  _Command Line Parameters_:  <br/>
  {{{
  --URLControlFile=./URLList.xml 
  --DirForObsKml=/home/xeniaprod/feeds/scout/general 
  --NetCDFDir=/home/xeniaprod/tmp/netcdf/scout 
  --FetchLogDir=/home/xeniaprod/tmp/netcdf/scout/fetch_logs
  --UseLastNTimeStamps=10 --UseLastNTimeStampsForOrg="nos;60,nws;20" > /home/xeniaprod/tmp/log/scout_netcdf.log

  --URLControlFile is the XML file with the URL list the script will download the files from.
  --FileFilter is the filter to apply to the file listings to allow the script to download the files we really want. Optional.
  --DirForObsKml provides the path to the store the ObsKML files created. This is an optional argument, if not provided no ObsKML files are written.
  --Delete specifies we delete the netcdf files after we write the ObsKML files. This is an optional argument.
  --NetCDFDir is the directory to store the downloaded files. Optional, default is ./latest_netcdf_files.
  --FetchLogDir is the directory were the file time stamps are stored. The script uses these files to determine which files are really the latest. Optional, default is ./fetch_logs
  --UseLastNTimeStamps Integer representing the last N time entries to use when converting the data to obsKML. Optional.
  --UseLastNTimeStampsForOrg Optional. List specifing orginization and the last N time entries to use for them. Whereas the UseLastNTimeStamps is applied to every organization, this can tailor to the specific. If not provided and UseLastNTimeStamps is provided, it is used.
  --SkipZipObsKmlDirs Optional. A value of 1 will skip the zipping of the directories under the DirForObsKml option.

  }}}
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
*Schedule*: Twice an hour
----

=== Carocoops NetCDF Scout ===
*Script*: populate_xenia_carocoops.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the Carocoops netcdf files. This script incoporates the NetCDF scout above, but only handles the Carocoops 
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Directory_: /home/xeniaprod/scripts/scout/trunk <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  {{{
  --URLControlFile=./CarocoopsURLList.xml 
  --DirForObsKml=/home/xeniaprod/feeds/scout/carocoops 
  --NetCDFDir=/home/xeniaprod/tmp/netcdf/scout 
  --FetchLogDir=/home/xeniaprod/tmp/netcdf/scout/fetch_logs
  --UseLastNTimeStamps=10
  }}}
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
  _Command Line Parameters_: <br/>
  {{{
  Argument 1 - The URL to the KMZ file to process.<br/>
  Argument 2 - Path where the SQL file will be written.<br/>
  Argument 3 - Filename to use for the SQL file.<br/>
  Argument 4 - Database name to connect to.<br/>

  http://localhost/xenia/feeds/carocoops/carocoops_metadata_latest.kmz 
  /home/xeniaprod/tmp/sqlfiles
  "latest_carocoops"
  "xenia"
  }}}
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
  
  
=== CORMP NetCDF Scout ===
*Script*: populate_xenia_cormp.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the CORMP netcdf files. This script incoporates the NetCDF scout above, but only handles the CORMP 
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Directory_: /home/xeniaprod/scripts/scout/trunk <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  {{{
  --URLControlFile=./CormpURLList.xml
  --DirForObsKml=/home/xeniaprod/feeds/scout/cormp 
  --NetCDFDir=/home/xeniaprod/tmp/netcdf/scout 
  --FetchLogDir=/home/xeniaprod/tmp/netcdf/scout/fetch_logs
  --UseLastNTimeStamps=10
  }}}
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
  _Command Line Parameters_: <br/>
  {{{
  Argument 1 - The URL to the KMZ file to process.<br/>
  Argument 2 - Path where the SQL file will be written.<br/>
  Argument 3 - Filename to use for the SQL file.<br/>
  Argument 4 - Database name to connect to.<br/>
  
  http://localhost/xenia/feeds/cormp/cormp_metadata_latest.kmz
  /home/xeniaprod/tmp/sqlfiles
  "latest_cormp"
  "xenia"
  }}}
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
 

=== USF NetCDF Scout ===
*Script*: populate_xenia_usf.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the USF and NDBC netcdf files. This script incoporates the NetCDF scout above, but only handles the USF and NDBC
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Directory_: /home/xeniaprod/scripts/scout/trunk <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  {{{
  --URLControlFile=./UsfURLList.xml
  --DirForObsKml=/home/xeniaprod/feeds/scout/usf 
  --NetCDFDir=/home/xeniaprod/tmp/netcdf/scout 
  --FetchLogDir=/home/xeniaprod/tmp/netcdf/scout/fetch_logs 
  --UseLastNTimeStamps=10
  }}}
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
  _Command Line Parameters_: <br/>
  {{{
  Argument 1 - The URL to the KMZ file to process.<br/>
  Argument 2 - Path where the SQL file will be written.<br/>
  Argument 3 - Filename to use for the SQL file.<br/>
  Argument 4 - Database name to connect to.<br/>
  
  For USF:
  http://localhost/xenia/feeds/usf/usf_metadata_latest.kmz
  /home/xeniaprod/tmp/sqlfiles
  "latest_usf"
  "xenia"

  For NDBC:
  http://localhost/xenia/feeds/ndbc/ndbc_metadata_latest.kmz
  /home/xeniaprod/tmp/sqlfiles
  "latest_ndbc"
  "xenia"
  }}}
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
         

=== NWS Processing ===
*Script*: populate_xenia_nws <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Pulls in NWS data directly from the NWS site. This is used in tandem with the NetCDF scout. Currently this is scheduled to run in crontab. <br/>
*Shell Script Makeup* <br/>
  _Script_: mk_sql_for_xenia.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/federal <br/>
  _Command line parameters_: <br/>
  {{{
  Argument 1: Provider name – nos, nws, etc <br/>
  Argument 2: Not used <br/>
  Argument 3: Fully qualified path to SQL output file <br/>
  Argument 4: Value of 'debug' will enable logging of some print   statements

  nws 
  24 
  /home/xeniaprod/tmp/sqlfiles/nws.sql
  debug
  }}}
  _Description_: Connects to the NWS webservice, pulls the data down into a SQL file for importation into the database. Next psql is executed to import the SQL 	file.<br/>
*Schedule*: Runs every 10 minutes. NWS stations have a fairly rapid update rate.
----

=== NOS Processing ===
*Script*: populate_xenia_nos <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Now using the IOOS Dif web service, pulls in NOS data directly from the NOS site. Currently this is scheduled to run in crontab. <br/>
*Shell Script Makeup* <br/>
  _Script_:DataIngestion.py <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/ioosdif <br/>
  _Command line parameters_: <br/>
  {{{
  --ConfigFile=/home/xeniaprod/config/nosDataIngest.ini  
  }}}
*Schedule*: Runs twice an hour.
----

=== USGS Processing ===
*Script*: populate_xenia_usgs <br/>
*Directory*: /home/xeniaprod/scripts/postgresql/feeds/federal/usgs<br/>
*Description*: Pulls in USGS data directly from the USGS site.  Currently this is scheduled to run in crontab. Cleans up previous KML, KMZ, and SQL files.<br/>
*Shell Script Makeup* <br/>
  _Script_: gen_wq_obskml.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/federal/usgs <br/>
  _Command line parameters_: <br/>
  {{{
  Argument 1: Provider name – usgs 
  Argument 2: Base output directory. Argument one is appended to complete the directory. 
  Argument 3: XML platform list that details the platforms to query.
  }}}
  _Description_: Connects to the USGS webservice, pulls the data down into a KML file for importation into the database.<br/>
 _Script_: obskml_to_xenia_postgresql.pl
 _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
 _Command Line Parameters_: <br/>
 {{{
 Argument 1 - The URL to the KMZ file to process.
 Argument 2 - Path where the SQL file will be written.
 Argument 3 - Filename to use for the SQL file.
 Argument 4 - Database name to connect to.
 }}}
 _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>

*Schedule*: 45 minutes after the hour.
----

=== FIT Processing ===
*Script*: feed_fit.sh <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Shell script that pulls in data from the Florida Institute of Technology<br/>
*Shell Script Makeup* <br/>
  _Script_: fit_to_obskml.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/fit <br/>
  _Command line parameters: <br/>
  {{{
  None
  Note: There are hardcoded paths and other settings that control where the data is retrieved and saved.
  URL: my $url = 'http://my.fit.edu/coastal/DATA.HTM';
  The text data is stored: "./latest.txt"
  The obsKml is stored: "./fit.kml"
  The kml file is then zipped and copied to: "/var/www/xenia/feeds/fit/fit_metadata_latest.kmz"
  }}}
  _Description_: Process the FIT data. <br/>

  _Script_: obskml_to_xenia_postgresql.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
  _Command_ line parameters: <br/>
  {{{
  Argument 1 - The URL to the KMZ file to process.
  Argument 2 - Path where the SQL file will be written.
  Argument 3 - Filename to use for the SQL file.
  Argument 4 - Database name to connect to.

  http://localhost/xenia/feeds/fit/fit_metadata_latest.kmz 
  ""
  "latest_in_situ" 
  "xenia"
  }}}
  _Description_: Takes the obsKML and writes it into the database. <br/>
*Schedule*: Runs every quarter hour.
----


=== SCCF Processing ===
*Script*: feed_sccf.sh <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Shell script that pulls in data from  Sanibel-Captiva Conservation Foundation River,Estuary and Coastal Observing Network in Florida. <br/>
*Shell Script Makeup* <br/>
  _Script_: sccf_to_obskml.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/sccf <br/>
  _Command line parameters: <br/>
  {{{
  None
  Note: There are hardcoded paths and other settings that control where the data is retrieved and saved.
  URL: my $url = 'http://recon.sccf.org/latest.kml';
  The text data is stored: "./latest.txt"
  The obsKml is stored: "./sccf.kml"
  The kml file is then zipped and copied to: "/var/www/xenia/feeds/sccf/sccf_metadata_latest.kmz"
  }}}
  _Description_: Process the SCCF data. <br/>

  _Script_: obskml_to_xenia_postgresql.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
  _Command_ line parameters: <br/>
  {{{
  Argument 1 - The URL to the KMZ file to process.
  Argument 2 - Path where the SQL file will be written.
  Argument 3 - Filename to use for the SQL file.
  Argument 4 - Database name to connect to.

  http://localhost/xenia/feeds/fit/fit_metadata_latest.kmz 
  ""
  "latest_in_situ" 
  "xenia"
  }}}
  _Description_: Takes the obsKML and writes it into the database. <br/>
*Schedule*: Runs every quarter hour.
----

=== FLDEP Processing ===
*Script*: feed_fldep.sh <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Shell script that pulls in data from the Florida Department of Environmental Protection <br/>
*Shell Script Makeup* <br/>
  _Script_: fldep_to_obskml.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/fldep <br/>
  _Command line parameters: <br/>
  {{{
  None
  Note: There are hardcoded paths and other settings that control where the data is retrieved and saved.
  URL: my $url = 'http://www.fldep-stevens.com/...';
  The text data is stored: "./latest.txt"
  The obsKml is stored: "./fldep.kml"
  The kml file is then zipped and copied to: "/var/www/xenia/feeds/fldep/fldep_metadata_latest.kmz"
  }}}
  _Description_: Process the FLDEP data. <br/>

  _Script_: obskml_to_xenia_postgresql.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
  _Command_ line parameters: <br/>
  {{{
  Argument 1 - The URL to the KMZ file to process.
  Argument 2 - Path where the SQL file will be written.
  Argument 3 - Filename to use for the SQL file.
  Argument 4 - Database name to connect to.

  http://localhost/xenia/feeds/fldep/fldep_metadata_latest.kmz 
  ""
  "latest_in_situ" 
  "xenia"
  }}}
  _Description_: Takes the obsKML and writes it into the database. <br/>
*Schedule*: Runs every 20 minutes.
----


=== YSI Apache Pier Processing ===
*Script*: getYSIData.py <br/>
*Directory*: /home/xeniaprod/scripts/postgresql/feeds/ysi <br/>
*Command line parameters*: 
{{{
  Argument 1: Fullpath to the XML configuration file.

  /home/xeniaprod/config/ysiParseConfig.xml
}}}
*Description*: Pulls in the data for the YSI site which is described in the configuration file 
provided  on the command line. This script pulls in the data for the Apache Pier. <br/>
*Schedule*: Runs twice an hour.
----
=== NERRS Processing ===
*Script*: populate_xenia_nerrs<br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Pulls in NERRS data directly from the NERRS site. This script has a command line parameter to call out how many records to request. This is needed since once a day a backfill request of 96(15 minute interval between records, so last 24 hours) as there is a downtime each time for the NERRS data telemytry.<br/>
*Shell Script Makeup* <br/>
  _Script_: filter_CDMO_Soap.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/nerrs<br/>
  _Command line parameters_: <br/>
  {{{
  Argument 1: Number of records to request 

  Hourly data:
  4
  Backfill:
  96
  }}}
 _Description_: Connects to the NERRS SOAP webservice requesting the observation data as XML. Each station has an xml file created.<br/>
	
  _Script_: gen_nerrs_obskml.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/feeds/nerrs<br/>
  _Command line parameters_: <br/>
  {{{
  Argument 1: Provider Name, nerrs  
  Argument 2: Output directory. Argument 1 is appended to it to form the complete path.  
  Argument 3: Platform list xml file.  
  Argument 4: Input file directory, should point to the location of the XML files created by the filter_CDMO_Soap.pl script above. 

  nerrs 
  /home/xeniaprod/feeds/scout 
  ./nerrs_platform_list.xml  
  /home/xeniaprod/feeds/scout/nerrs/soap 
  }}}
  _Description_: Creates the obsKML files by parsing the XML files created by filter_CDMO_Soap.pl. <br/>
  
  _Script_: obskml_to_xenia_postgresql.pl <br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
  _Command Line Parameters_: <br/>
  {{{
  Argument 1 - The URL to the KMZ file to process.
  Argument 2 - Path where the SQL file will be written.
  Argument 3 - Filename to use for the SQL file.
  Argument 4 - Database name to connect to.

  http://localhost/xenia/feeds/nerrs/nerrs_metadata_latest.kmz 
  /home/xeniaprod/tmp/sqlfiles 
  "latest_nerrs" 
  "xenia"
  }}}
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>

*Schedule*: Runs twice an hour.
----
=== YSI Nerrs Processing ===
*Script*: getYSIData.py <br/>
*Directory*: /home/xeniaprod/scripts/postgresql/feeds/ysi <br/>
*Command line parameters*: 
{{{
  Argument 1: Fullpath to the XML configuration file.

  /home/xeniaprod/config/nerrsYSIConfigLinux.xml
}}}
*Directory*: /home/xeniaprod/ /scripts/postgresql/feeds/ysi <br/>
*Description*: Pulls in the data for the YSI site which is described in the configuration file 
provided  on the command line. This script pulls in the data for the some NERRS water quality sites. <br/>
*Schedule*: Runs twice an hour.
----

= Machine: Mapping =
http://129.252.139.139/

== Scripts: Mapping ==

----

=== MODIS Sea Surface Temperature === 
*Script*: get_usf_modis_sst.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*:  Gets the latest modis file and updates the Xenia database on neptune.<br/>
*Shell Script Makeup* <br/>
  _Script_:  get_latest_data.pl<br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/remotesensing/modis_sst <br/>
  _Command line parameters_: 
  {{{
  None. 
  NOTE: There are a number of hardcoded settings in the script, such as working directorys and the URLs to connect to for the data.       
  }}}
  The following are the hardcoded working directories:
  {{{
  my $scratch_dir = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name;
  my $dest_dir    = '/home/xeniaprod/feeds/remotesensing/'.$layer_name;  
  my $fetch_logs   = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name.'/fetch_logs';    
  }}}
  The following are the hardcoded URLS:
  {{{
  @dir_urls = (
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$yesterdayJulian/1km/pass/intermediate/",
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$yesterdayJulian/1km/pass/final/",
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$todayJulian/1km/pass/intermediate/",
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$todayJulian/1km/pass/final/"
  );    
  }}}
  The $product_id variable is hardcoded to the value from the product_type table for the data we want. See the _Description_ below.

  The $psql_command is also hardcoded to connect to the database to import the SQL statement.<br/>      
      
  _Description_: Gets the modis file and updates the Xenia database on neptune. The files are downloaded then the file name modified to the format of modis_sst_year_month_day_hour_minute.png then moved to the $dest_dir described above. This is done to have a consistent file naming scheme across the products. The database table used is "timestamp_lkp" which is a generalized table for the remote sensing products. For a given data layer, a mappping product would query the database for the specific product id's(layer's) most recent time entry. The database would return the timestamp which would then be appended to the layer name to then get access to the most recent file. The table column layout is as follows:
  {{{
  row_id - Autoincrementing integer as the primary key
  row_entry_date - Date when the row was added to the table
  row_update_date - Date the row was modified last.
  product_id - Integer that specifies what product the row is for.  These are described in the product_type table. Modis is id 1.
  pass_timestamp - the products date, the time the date was taken.
  filepath - the child path to the file. The user would have to know the parent directory, such as /home/xeniaprod/feeds/remotesensing, then the filepath would be appended to that to then get the requested file.
  }}}

*Schedule*: Top of each hour
----
=== Interpolated Remote Sea Surface Temperature === 
*Script*: get_usf_modis_sst.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*:  Gets the latest interpolated SST file and updates the Xenia database on neptune.<br/>
*Shell Script Makeup* <br/>
  _Script_:  get_latest_data.pl<br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/remotesensing/modis_sst <br/>
  _Command line parameters_: 
  {{{
  None. 
  NOTE: There are a number of hardcoded settings in the script, such as working directorys and the URLs to connect to for the data.       
  }}}
  The following are the hardcoded working directories: <br/>
  {{{
    my $scratch_dir = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name;
    my $dest_dir    = '/home/xeniaprod/feeds/remotesensing/'.$layer_name;  
    my $fetch_logs   = '/home/xeniaprod/tmp/remotesensing/usf/oi_sst/fetch_logs';
  }}}
  The following are the hardcoded URLS: <br/>
  {{{
  @dir_urls = (
    'http://ocgweb.marine.usf.edu/Products/OI/oidat/'
  );    
  }}}
  The $product_id variable is hardcoded to the value from the product_type table for the data we want. See the _Description_ below.

  The $psql_command is also hardcoded to connect to the database to import the SQL statement.
  _Description_: Gets the modis file and updates the Xenia database on neptune. The files are downloaded then the file name modified to the format of oi_sst_year_month_day_hour_minute.png then moved to the $dest_dir described above. This is done to have a consistent file naming scheme across the products. The database table used is "timestamp_lkp" which is a generalized table for the remote sensing products. For a given data layer, a mappping product would query the database for the specific product id's(layer's) most recent time entry. The database would return the timestamp which would then be appended to the layer name to then get access to the most recent file.
The database would return the timestamp which would then be appended to the layer name to then get access to the most recent file. The table column layout is as follows:
  {{{
  row_id - Autoincrementing integer as the primary key
  row_entry_date - Date when the row was added to the table
  row_update_date - Date the row was modified last.
  product_id - Integer that specifies what product the row is for. These are described in the product_type table. Modis is id 1.
  pass_timestamp - the products date, the time the date was taken.
  filepath - the child path to the file. The user would have to know the parent directory, such as /home/xeniaprod/feeds/remotesensing, then the filepath would be appended to that to then get the requested file.
  }}}

*Schedule*: Top of each hour

----
=== Advanced Very High Resolution Radiometer Sea Surface Temperature === 
*Script*: get_usf_avhrr_sst.sh.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*:  Gets the latest AVHRR SST file and updates the Xenia database on neptune. Also updates the old schema database to feed
the static maps for SECOORA generated at Chapel Hill.<br/>
*Shell Script Makeup* <br/>
  _Script_:  get_latest_data.pl<br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/remotesensing/avhrr_sst <br/>
  _Command line parameters_: 
  {{{
  None. 
  NOTE: There are a number of hardcoded settings in the script, such as working directorys and the URLs to connect to for the data.       
  }}}
  The following are the hardcoded working directories: <br/>
  {{{
    my $scratch_dir = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name;
    my $dest_dir    = '/home/xeniaprod/feeds/remotesensing/'.$layer_name;  
    my $dest_dir_2    = '/nautilus_usr2/maps/seacoos/data/usf/'.$layer_name;  
  }}}
  The following are the hardcoded URLS: <br/>
  {{{
    @dir_urls = (
      'http://www.imars.usf.edu/husf_avhrr/products/images/fullpass/'.$yyyy_dot_mm
    );

    @final_dods_urls = (
      'http://www.imars.usf.edu/dods-bin/nph-dods/husf_avhrr/FULL_PASS_HDF_SST/'.$yyyy_dot_mm
    );
    $final_suffix = 'usf.sst.hdf.html';

    @auto_dods_urls = (
      'http://www.imars.usf.edu/dods-bin/nph-dods/husf_avhrr/FULL_PASS_HDF_SST/auto/'.$yyyy_dot_mm
    );
  );    
  }}}
  The $product_id variable is hardcoded to the value from the product_type table for the data we want. See the _Description_ below.

  The $psql_command is also hardcoded to connect to the database to import the SQL statement.
  _Description_: Gets the modis file and updates the Xenia database on neptune. The files are downloaded then the file name modified to the format of oi_sst_year_month_day_hour_minute.png then moved to the $dest_dir described above. This is done to have a consistent file naming scheme across the products. The database table used is "timestamp_lkp" which is a generalized table for the remote sensing products. For a given data layer, a mappping product would query the database for the specific product id's(layer's) most recent time entry. The database would return the timestamp which would then be appended to the layer name to then get access to the most recent file.
The database would return the timestamp which would then be appended to the layer name to then get access to the most recent file. The table column layout is as follows:
  {{{
  row_id - Autoincrementing integer as the primary key
  row_entry_date - Date when the row was added to the table
  row_update_date - Date the row was modified last.
  product_id - Integer that specifies what product the row is for. These are described in the product_type table. Modis is id 1.
  pass_timestamp - the products date, the time the date was taken.
  filepath - the child path to the file. The user would have to know the parent directory, such as /home/xeniaprod/feeds/remotesensing, then the filepath would be appended to that to then get the requested file.
  }}}
  This script also feeds entries to the old schema database. In that database there is a table per remote sensing product.
*Schedule*: 30 minutes after the hour

----
=== MODIS RGB True Color === 
*Script*: get_usf_modis_rgb.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*:  Gets the latest MODIS RGB file and updates the Xenia database on neptune. Also updates the old schema database to feed
the static maps for SECOORA generated at Chapel Hill.<br/>
*Shell Script Makeup* <br/>
  _Script_:  get_latest_data.pl<br/>
  _Directory_: /home/xeniaprod/scripts/postgresql/remotesensing/modis_rgb <br/>
  _Command line parameters_: 
  {{{
  None. 
  NOTE: There are a number of hardcoded settings in the script, such as working directorys and the URLs to connect to for the data.       
  }}}
  The following are the hardcoded working directories: <br/>
  {{{
    my $scratch_dir = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name;
    my $dest_dir    = '/home/xeniaprod/feeds/remotesensing/'.$layer_name;  
    my $dest_dir_2    = '/nautilus_usr2/maps/seacoos/data/usf/'.$layer_name;  
  }}}
  The following are the hardcoded URLS: <br/>
  {{{
  @dir_urls = (
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$yesterdayJulian/1km/pass/intermediate/",
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$yesterdayJulian/1km/pass/final/",
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$todayJulian/1km/pass/intermediate/",
    "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$todayJulian/1km/pass/final/"
    #'http://modis.marine.usf.edu/products/fullpass/rgb/'
  );
  }}}
  The $product_id variable is hardcoded to the value from the product_type table for the data we want. See the _Description_ below.

  The $psql_command is also hardcoded to connect to the database to import the SQL statement.
  _Description_: Gets the modis file and updates the Xenia database on neptune. The files are downloaded then the file name modified to the format of oi_sst_year_month_day_hour_minute.png then moved to the $dest_dir described above. This is done to have a consistent file naming scheme across the products. The database table used is "timestamp_lkp" which is a generalized table for the remote sensing products. For a given data layer, a mappping product would query the database for the specific product id's(layer's) most recent time entry. The database would return the timestamp which would then be appended to the layer name to then get access to the most recent file.
The database would return the timestamp which would then be appended to the layer name to then get access to the most recent file. The table column layout is as follows:
  {{{
  row_id - Autoincrementing integer as the primary key
  row_entry_date - Date when the row was added to the table
  row_update_date - Date the row was modified last.
  product_id - Integer that specifies what product the row is for. These are described in the product_type table. Modis is id 1.
  pass_timestamp - the products date, the time the date was taken.
  filepath - the child path to the file. The user would have to know the parent directory, such as /home/xeniaprod/feeds/remotesensing, then the filepath would be appended to that to then get the requested file.
  }}}
  This script also feeds entries to the old schema database. In that database there is a table per remote sensing product.
*Schedule*: 35 minutes after the hour

----
=== Remote Sensing Cache Reseed === 
*Script*: rs_clean_reseed.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*: Uses the tilecache scripts tilecache_clean.py and tilecache_seed.py to flush the tile cache then reseed the primary zoom level(the view a user first sees on the map). Tilecaching the remote sensing layers allows us to serve up the layers quicker than constantly hitting mapserver. <br/>
NOTE: This jobs runs under the www-data cron tab since the incoming layer requests are handled by this users. Otherwise we'd run into file/directory permission issues. <br/>
If the tilecache root directory is changed in the tilecache.cfg file, the directory for the clean and seed scripts would need to be changed in the shell script as well. Currently the root directory is: /usr2/data/xeniaprod/tmp/mapserver_tmp/tilecache/<br/>

*Command line parameters*: None <br/>
*Schedule*: 30 minutes after the hour. We could refine this by having the recaching done whenever we actually get new remote sensing layers.

----
=== Hourly Observation Cache Reseed === 
*Script*: hourly_obs_clean_reseed.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*: Uses the tilecache scripts tilecache_clean.py and tilecache_seed.py to flush the tile cache then reseed the primary zoom level(the view a user first sees on the map). Tilecaching the hourly observation visualization layers allows us to serve up the layers quicker than constantly hitting mapserver. 
NOTE: This jobs runs under the www-data cron tab since the incoming layer requests are handled by this users. Otherwise we'd run into file/directory permission issues. <br/>
If the tilecache root directory is changed in the tilecache.cfg file, the directory for the clean and seed scripts would need to be changed in the shell script as well. Currently the root directory is: /usr2/data/xeniaprod/tmp/mapserver_tmp/tilecache/<br/>

*Command line parameters*: None <br/>
*Schedule*: 10&50 minutes after the hour. 

----
=== HF Radar === 

These scripts run once an hour to help aggregate and check uptime for the Secoora HF Radar stations.  The final rendering of these hf radar netcdf files is currently handled by servers at NC-Chapel Hill.

*directory* /home/xeniaprod/scripts/radar

The directory itself contains scripts for getting Savannah HF radar(get_latest_listing.pl,get_latest_data.pl,convert_latest.pl,cronNotify.pl)

The Savannah script checks a remote http folder for the latest (downloaded as latest.txt) data file which gets converted into a seacoos/secoora netcdf grid file using a netcdf template/substitution file.

For Tampa/West Florida Shelf(WFS), a similar set of scripts is under the 'tampa' folder.

For Miami, a similar set of scripts is under 'miami'.

For NC Outer banks, a similar set of scripts is under 'banks' - for 'banks' no data is collected or converted, just uptime/notification checks.