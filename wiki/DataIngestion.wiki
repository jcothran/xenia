#summary Scripts that create and import the data into the Xenia database.

<wiki:toc max_depth="4" />

= Introduction =

These are the scripts currently running that retrieve the data from various sources and import that data into the Xenia database.<br/>


= Machine: Neptune = 
neptune.baruch.sc.edu <br/>


== Scripts Neptune ==
----
=== NetCDF Scout ===
*Script*: populate_xenia_netcdfscout.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing netcdf files that aren't handled through the other RA specific processing scripts below. This script incoporates the NetCDF scout above, but only handles the Carocoops 
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Twice an hour
----

=== Carocoops NetCDF Scout ===
*Script*: populate_xenia_carocoops.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the Carocoops netcdf files. This script incoporates the NetCDF scout above, but only handles the Carocoops 
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
  
  
=== CORMP NetCDF Scout ===
*Script*: populate_xenia_cormp.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the CORMP netcdf files. This script incoporates the NetCDF scout above, but only handles the CORMP 
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
 

=== USF NetCDF Scout ===
*Script*: populate_xenia_usf.sh<br/>
*Command line parameters*:<br/>
*Directory*: /home/xeniaprod/cron<br/>
*Description*: Shell script for processing the USF and NDBC netcdf files. This script incoporates the NetCDF scout above, but only handles the USF and NDBC
netcdf files. The files are pulled down then processed into the database. Deletes the previous KML, ZIP and KMZ files.<br/>
*Shell Script Makeup* <br/>
  _Script_: GetLatestData.pl <br/>
  _Command Line Parameters_: See NetCDF section above. <br/>
  _Description_: Processes the netCDF files into obsKML files.<br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>
*Schedule*: Runs every quarter hour. We run it this way since the dial up times for the platforms occur throughout the hour, one may be dialed at
5 past the hour and another at 20 past.
----
         

=== NWS Processing ===
*Script*: populate_xenia_nws <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Pulls in NWS data directly from the NWS site. This is used in tandem with the NetCDF scout. Currently this is scheduled to run in crontab. <br/>
*Shell Script Makeup* <br/>
	_Script_: mk_sql_for_xenia.pl <br/>
	_Command line parameters_: <br/>
		Argument 1: Provider name – nos, nws, etc <br/>
		Argument 2: Not used <br/>
		Argument 3: Fully qualified path to SQL output file <br/>
	_Description_: Connects to the NWS webservice, pulls the data down into a SQL file for importation into the database. Next psql is executed to import the SQL 	file.<br/>
*Schedule*: Runs every 10 minutes. NWS stations have a fairly rapid update rate.
----


=== NOS Processing ===
*Script*: populate_xenia_nos <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Pulls in NOS data directly from the NOS site. This is used in tandem with the NetCDF scout. Currently this is scheduled to run in crontab. <br/>
*Shell Script Makeup* <br/>
	_Script_: mk_sql_for_xenia.pl <br/>
	_Command line parameters_: <br/>
		Argument 1: Provider name – nos, nws, etc <br/>
		Argument 2: Not used <br/>
		Argument 3: Fully qualified path to SQL output file	<br/>
	_Description_: Connects to the NOS webservice, pulls the data down into a SQL 	file for importation into the database. Next psql is executed to import the SQL 	file. <br/>
*Schedule*: Runs twice an hour.
----

=== USGS Processing ===
*Script*: populate_xenia_usgs <br/>
*Directory*: /home/xeniaprod/scripts/postgresql/feeds/federal/usgs<br/>
*Description*: Pulls in USGS data directly from the USGS site.  Currently this is scheduled to run in crontab. Cleans up previous KML, KMZ, and SQL files.<br/>
*Shell Script Makeup* <br/>
	_Script_: gen_wq_obskml.pl <br/>
	_Command line parameters_: <br/>
		Argument 1: Provider name – usgs <br/>
		Argument 2: Base output directory. Argument one is appended to complete the directory. <br/>
		Argument 3: XML platform list that details the platforms to query.<br/>
	_Description_: Connects to the USGS webservice, pulls the data down into a KML 	file for importation into the database.<br/>
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
    <br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>

*Schedule*: 45 minutes after the hour.
----

=== FIT Processing ===
*Script*: feed_fit.sh <br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Shell script that pulls in data from the Sebastion Inlet station in Florida. <br/>
*Shell Script Makeup* <br/>
	_Script_: fit_to_obskml.pl <br/>
	_Directory_: /home/xeniaprod/scripts/postgresql/feeds/fit <br/>
	_Description_: Process the Fit data. <br/>
	_Script_: obskml_to_xenia_postgresql.pl <br/>
	_Directory_: /home/xeniaprod/scripts/postgresql/import_export <br/>
	_Description_: Takes the obsKML and writes it into the database. <br/>
*Schedule*: Runs every quarter hour.
----


=== YSI Apache Pier Processing ===
*Script*: getYSIData.py <br/>
*Command line parameters*: <br/>
	Argument 1: /home/xeniaprod/config/ysiParseConfig.xml <br/>
*Directory*: /home/xeniaprod/ /scripts/postgresql/feeds/ysi <br/>
*Description*: Pulls in the data for the YSI site which is described in the configuration file 
provided  on the command line. This script pulls in the data for the Apache Pier. <br/>
*Schedule*: Runs twice an hour.
----
=== NERRS Processing ===
*Script*: populate_xenia_nerrs<br/>
*Directory*: /home/xeniaprod/cron <br/>
*Description*: Pulls in NERRS data directly from the NERRS site. <br/>
*Shell Script Makeup* <br/>
	_Script_: filter_CDMO_Soap.pl <br/>
	_Command line parameters_: <br/>
	  Argument 1: Number of records to request <br/>
	_Description_: Connects to the NERRS SOAP webservice requesting the observation data as XML. Each station has an xml file created.<br/>
	
  _Script_: gen_nerrs_obskml.pl <br/>
	_Command line parameters_: <br/>
    Argument 1: Provider Name, nerrs  <br/>
    Argument 2: Output directory. Argument 1 is appended to it to form the complete path.  <br/>
    Argument 3: Platform list xml file.  <br/>
    Argument 4: Input file directory, should point to the location of the XML files created by the filter_CDMO_Soap.pl script above. <br/>
	_Description_: Creates the obsKML files by parsing the XML files created by filter_CDMO_Soap.pl. <br/>
  
  _Script_: obskml_to_xenia_postgresql.pl
  _Command Line Parameters_: <br/>
    Argument 1 - The URL to the KMZ file to process.<br/>
    Argument 2 - Path where the SQL file will be written.<br/>
    Argument 3 - Filename to use for the SQL file.<br/>
    Argument 4 - Database name to connect to.<br/>
  _Description_: Processes the obsKML files writing them into a sql file that is then imported into the database.<br/>

*Schedule*: Runs twice an hour.
----
=== YSI Nerrs Processing ===
*Script*: getYSIData.py <br/>
*Command line parameters*: <br/>
	Argument 1: /home/xeniaprod/config/nerrsYSIConfigLinux.xml <br/>
*Directory*: /home/xeniaprod/ /scripts/postgresql/feeds/ysi <br/>
*Description*: Pulls in the data for the YSI site which is described in the configuration file 
provided  on the command line. This script pulls in the data for the some NERRS water quality sites. <br/>
*Schedule*: Runs twice an hour.
----

= Machine: Mapping =
http://129.252.139.139/

== Scripts: Mapping ==

----

=== MODIS Sea Surface Temperature === 
*Script*: get_usf_modis_sst.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*:  Gets the latest modis file and updates the Xenia database on neptune.<br/>
*Shell Script Makeup* <br/>
	_Script_:  get_latest_data.pl<br/>
	_Command line parameters_: 
  {{{
    None. 
    NOTE: There are a number of hardcoded settings in the script, such as working directorys and the URLs to connect to for the data. 
      
  }}}
	_Description_: Gets the modis file and updates the Xenia database on neptune.<br/>
      The following are the hardcoded working directories: <br/>
      {{{
      my $scratch_dir = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name;
      my $dest_dir    = '/home/xeniaprod/feeds/remotesensing/'.$layer_name;  
      my $fetch_logs   = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name.'/fetch_logs';    
      }}}
      The following are the hardcoded URLS: <br/>
      {{{
      @dir_urls = (
        "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$yesterdayJulian/1km/pass/intermediate/",
        "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$yesterdayJulian/1km/pass/final/",
        "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$todayJulian/1km/pass/intermediate/",
        "http://cyclops.marine.usf.edu/modis/level3/husf/fullpass/$currentYear/$todayJulian/1km/pass/final/"
        #'http://modis.marine.usf.edu/products/fullpass/sst/'
      );    
      }}}
      The $psql_command is also hardcoded to connect to the database to import the SQL statement.

*Schedule*: Top of each hour
----
=== Interpolated Remote Sea Surface Temperature === 
*Script*: get_usf_modis_sst.sh<br/>
*Directory*:  /home/xeniaprod/cron<br/>
*Description*:  Gets the latest interpolated SST file and updates the Xenia database on neptune.<br/>
*Shell Script Makeup* <br/>
	_Script_:  get_latest_data.pl<br/>
	_Command line parameters_: 
  {{{
    None. 
    NOTE: There are a number of hardcoded settings in the script, such as working directorys and the URLs to connect to for the data. 
      
  }}}
	_Description_: Gets the latest interpolated SST file and updates the Xenia database on neptune.<br/>
      The following are the hardcoded working directories: <br/>
      {{{
        my $scratch_dir = '/home/xeniaprod/tmp/remotesensing/usf/'.$layer_name;
        my $dest_dir    = '/home/xeniaprod/feeds/remotesensing/'.$layer_name;  
        my $fetch_logs   = '/home/xeniaprod/tmp/remotesensing/usf/oi_sst/fetch_logs';
      }}}
      The following are the hardcoded URLS: <br/>
      {{{
      @dir_urls = (
        'http://ocgweb.marine.usf.edu/Products/OI/oidat/'
      );    
      }}}
      The $psql_command is also hardcoded to connect to the database to import the SQL statement.

*Schedule*: Top of each hour